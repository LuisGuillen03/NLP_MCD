
@inproceedings{joachims1998,
  author = {Joachims, T.},
  title = {Text categorization with support vector machines: learning with many relevant features},
  booktitle = {Proceedings of the 10th European Conference on Machine Learning (ECML)},
  year = {1998},
  pages = {137--142},
  doi = {10.1007/BFb0026683}
}

@article{breiman2001,
  author = {Breiman, L.},
  title = {Random Forests},
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  year = {2001},
  doi = {10.1023/A:1010933404324}
}

@article{he2009,
  author = {He, H. and Garcia, E. A.},
  title = {Learning from imbalanced data},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {21},
  number = {9},
  pages = {1263--1284},
  year = {2009},
  doi = {10.1109/TKDE.2008.239}
}

@article{sanh2019,
  author = {Sanh, V. and Debut, L. and Chaumond, J. and Wolf, T.},
  title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  journal = {arXiv preprint arXiv:1910.01108},
  year = {2019},
  doi = {10.48550/arXiv.1910.01108}
}

@inproceedings{devlin2019,
  author = {Devlin, J. and Chang, M.-W. and Lee, K. and Toutanova, K.},
  title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)},
  pages = {4171--4186},
  year = {2019},
  doi = {10.48550/arXiv.1810.04805}
}

@inproceedings{brown2020,
  author = {Brown, T. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. D. and Dhariwal, P. and Neelakantan, A. and Shyam, P. and Sastry, G. and Askell, A. and Herbert-Voss, A. and others},
  title = {Language Models are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  year = {2020},
  doi = {10.48550/arXiv.2005.14165}
}

@inproceedings{kim2014,
  author = {Kim, Y.},
  title = {Convolutional neural networks for sentence classification},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
  pages = {1746--1751},
  year = {2014},
  doi = {10.48550/arXiv.1408.5882}
}

@article{vaswani2017,
  author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A. N. and Kaiser, L. and Polosukhin, I.},
  title = {Attention is all you need},
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  pages = {5998--6008},
  year = {2017},
  doi = {10.48550/arXiv.1706.03762}
}

@article{pennington2014,
  author = {Pennington, J. and Socher, R. and Manning, C. D.},
  title = {GloVe: Global vectors for word representation},
  journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
  pages = {1532--1543},
  year = {2014},
  doi = {10.3115/v1/D14-1162}
}

@article{pedregosa2012,
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and others},
  title = {Scikit-learn: Machine Learning in Python},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  year = {2011},
  doi = {10.48550/arXiv.1201.0490}
}

@article{ghourabi2023,
  author = {Ghourabi, A. and Alohaly, M.},
  title = {Enhancing spam message classification and detection using transformer-based embedding and ensemble learning},
  journal = {Sensors},
  volume = {23},
  number = {8},
  pages = {3861},
  year = {2023},
  doi = {10.3390/s23083861}
}

@article{oyeyemi2023,
  author = {Oyeyemi, D. A. and Ojo, A. K.},
  title = {SMS spam detection and classification to combat abuse in telephone networks using natural language processing},
  journal = {Journal of Advances in Mathematics and Computer Science},
  volume = {38},
  number = {10},
  pages = {144--156},
  year = {2023},
  doi = {10.9734/JAMCS/2023/v38i101832}
}

@article{almeida2013,
  author = {Almeida, Tiago A. and Hidalgo, J. M. Gomez and Silva, Tiago P.},
  title = {SMS Spam Collection},
  journal = {UCI Machine Learning Repository},
  year = {2012},
  howpublished = {\url{https://archive.ics.uci.edu/dataset/228/sms+spam+collection}},
  note = {Dataset accessed 2025-10-05},
  doi = {10.24432/C5CC84}
}

@article{liu2021,
  author = {Liu, Xiaoxu and Lu, Haoye and Nayak, Amiya},
  title = {A Spam Transformer Model for SMS Spam Detection},
  journal = {IEEE Access},
  volume = {PP},
  pages = {1--1},
  year = {2021},
  doi = {10.1109/ACCESS.2021.3081479}
}

@article{alkaabi2024,
  author = {Al-Kaabi, Hussein and Darroudi, Ali and Jasim, Ali Kadhim},
  title = {Survey of SMS Spam Detection Techniques: A Taxonomy},
  journal = {AlKadhim Journal for Computer Science},
  volume = {2},
  number = {4},
  pages = {23--34},
  year = {2024},
  doi = {10.61710/kjcs.v2i4.88}
}

@article{bilgen2024,
  author = {Bilgen, Yusuf and Kaya, Mahmut},
  title = {EGMA: Ensemble Learning-Based Hybrid Model Approach for Spam Detection},
  journal = {Applied Sciences},
  volume = {14},
  number = {21},
  pages = {9669},
  year = {2024},
  doi = {10.3390/app14219669}
}

@article{kmail2025,
  author = {Kmail, Tahany and Hawa, Marah and Hasasneh, Ahmad},
  title = {Spam Detection Using an Advanced Hybrid Model},
  journal = {Journal of Advances in Information Technology},
  volume = {16},
  number = {9},
  pages = {1318--1328},
  year = {2025},
  doi = {10.12720/jait.16.9.1318-1328}
}

@article{hochreiter1997,
    author = {Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
    title = {Long Short-Term Memory},
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}
